## 📚 References for course-5

Below is a curated collection of resources to deepen your understanding of importance sampling, Bayesian inference, and generative modeling. These references span foundational texts, practical tutorials, framework documentation, research papers, and community hubs—mirroring the structure of Course 5’s reference folder.

---

### 📖 Fundamental Academic Works  
- **The Master Algorithm** by Pedro Domingos (2015)  
  A broad survey of machine learning paradigms, framing generative vs. discriminative approaches.  
  https://www.amazon.com/Master-Algorithm-Ultimate-Learning-Machine/dp/0465065706  
- **Deep Learning** by Goodfellow, Bengio & Courville (2016)  
  Chapters on probability, variational methods, and optimization provide theoretical underpinnings for importance sampling.  
  https://www.deeplearningbook.org/  
- **Handwritten Digit Recognition with a Back-Propagation Network** (LeCun et al., 1989)  
  The original CNN paper that illustrates how generative thinking (data augmentation, sampling) improves robustness.  
  https://www.researchgate.net/publication/2437764_Handwritten_Digit_Recognition_with_a_Back-Propagation_Network  

---

### ⚙️ Optimization Classics  
- **Adam: A Method for Stochastic Optimization** (Kingma & Ba, 2014)  
  Introduces Adam optimizer—essential for training proposal networks and VAEs.  
  https://arxiv.org/abs/1412.6980  
- **Batch Normalization** (Ioffe & Szegedy, 2015)  
  Stabilizes training, reducing variance in Monte Carlo estimates of network gradients.  
  https://arxiv.org/abs/1502.03167  

---

### 🎥 Video Courses & Lectures  
- **Stanford CS231n: Convolutional Neural Networks** (2017)  
  Lectures on optimization and probabilistic interpretations of deep models.  
  https://cs231n.github.io/  
- **Stanford CS224n: NLP with Deep Learning**  
  Covers sequence modeling and sampling-based inference in language models.  
  http://web.stanford.edu/class/cs224n/  
- **DeepLearning.AI TensorFlow Specialization**  
  Practical walkthroughs on TensorFlow probability and sampling layers.  
  https://www.coursera.org/specializations/tensorflow-in-practice  

---

### 📚 Framework Documentation  
- **TensorFlow Guide: Convolutional Neural Networks**  
  Tutorial on building and sampling from probabilistic models with TF 2.x.  
  https://www.tensorflow.org/tutorials/images/cnn  
- **Keras Applications**  
  Pretrained models and examples of using importance weights in transfer learning.  
  https://keras.io/api/applications/  
- **TensorFlow RNN Tutorial**  
  Demonstrates sequence sampling techniques applicable to time-series generative models.  
  https://www.tensorflow.org/guide/keras/rnn  
- **PyTorch Tutorials: Deep Learning with PyTorch**  
  Covers custom sampler implementations and weight normalization in PyTorch.  
  https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html  
- **PyTorch Lightning Documentation**  
  Best practices for scalable sampling-based experiments.  
  https://pytorch-lightning.readthedocs.io/  

---

### 🛠 Interactive Platforms & Experiments  
- **TensorFlow Playground**  
  Browser-based toy models to visualize sampling and weight effects.  
  https://playground.tensorflow.org/  
- **ConvNetJS by Karpathy**  
  Train small networks in-browser and experiment with sampling strategies.  
  https://cs.stanford.edu/people/karpathy/convnetjs/  
- **Google Colab Tutorials**  
  Ready-to-run notebooks on probabilistic modeling and importance sampling.  
  https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/  

---

### ☁ Cloud Experimentation  
- **Google Cloud AI Platform**  
  Scalable environment for Monte Carlo and sampling-intensive workloads.  
  https://cloud.google.com/ai-platform  
- **AWS SageMaker**  
  Managed infrastructure for distributed importance sampling experiments.  
  https://aws.amazon.com/sagemaker/  

---

### 📄 Research Papers & Preprints  
- **AdamW: Decoupled Weight Decay Regularization** (Loshchilov & Hutter, 2017)  
  Improved optimizer for stable sampling in high-dimensional models.  
  https://arxiv.org/abs/1711.05101  
- **AdaBelief Optimizer** (2020)  
  Alternative to Adam offering lower variance in weight updates.  
  https://arxiv.org/abs/2010.07468  
- **Attention Is All You Need** (Vaswani et al., 2017)  
  Transformer architecture—sampling-based generation at the heart of LLMs.  
  https://arxiv.org/abs/1706.03762  

---

### 🤝 Communities & Continuous Learning  
- **r/MachineLearning**  
  Ongoing discussion of Monte Carlo methods and Bayesian inference.  
  https://www.reddit.com/r/MachineLearning/  
- **TensorFlow Forum**  
  Q&A on sampling implementations and probabilistic layers.  
  https://discuss.tensorflow.org/  
- **Papers with Code**  
  Search implementations of importance sampling and generative models.  
  https://paperswithcode.com/area/deep-learning  

---

### 🚀 Learning Progression & Career Development  
- **TensorFlow Developer Certificate**  
  Validates hands-on skills in probabilistic modeling and sampling.  
  https://www.tensorflow.org/certificate  
- **AWS Certified Machine Learning – Specialty**  
  Demonstrates expertise in scalable inference and generative techniques.  
  https://aws.amazon.com/certification/certified-machine-learning-specialty/  
- **Distill.pub** & **The Gradient**  
  Interactive explainers and commentary on advanced sampling and generative research.  
  https://distill.pub/  
  https://thegradient.pub/  

---

> This reference collection draws from Course 5’s curated resources. Dive into these materials to strengthen your theoretical foundation, explore practical implementations, and join vibrant communities focused on generative AI and probabilistic modeling.
